# -*- coding: utf-8 -*-
"""
Created on Wed Nov  7 10:00:55 2018

@author: Akitaka
"""

[1b] 水素化物の超伝導予測
[1b1] ファイル統一過去のまとめ
C:\Users\Akitaka\Downloads\python\1026\Tc_4model_AD_DCV.py
20181026:EN, RR, LASSO, kNN

C:\Users\Akitaka\Downloads\python\1105\Tc_6model_AD_DCV.py
20181105: ...+RF, SVR

[1b2]
ENのαとλは範囲が違う？αは0も1も取り得る
https://datachemeng.com/rrlassoen/
しかし、実際に0を範囲に含めると、エラーというか警告文で「収束しにくい」と言われる
その為、1のみを含めることにした。


[1b3] ガウス過程回帰
20180829より引用<div>
ガウス過程による回帰(Gaussian Process Regression, GPR)
～予測値だけでなく予測値のばらつきも計算できる！～
https://datachemeng.com/gaussianprocessregression/
ref:05/17, 05/24
線形の回帰分析手法
カーネルトリックにより非線形の回帰モデルに
目的変数の推定値だけでなく、その分散も計算できる
クロスバリデーションがいらない
----
前提：学習データの数nに対し、n+1個目のデータの目的変数yを推定したい
線形モデル y= X*b
bは、1つの値ではなく正規分布と仮定する（xは値）
xが近いサンプル同士は、yも近いだろう
→サンプル間におけるyの値の関係(A)は、xの値の関係から計算できるだろう
b同様、yも正規分布と仮定する（n個の正規分布）
A＝正規分布同士の関係＝共分散 を求める
　bの平均=0,分散σ^2とすると、
　mi = yiの平均 = xi*(bの平均) = 0
　σij^2 = yiとyjの共分散 = xi*xj*(bとbの共分散) = xi*xj*(bの分散)　= xi*xj*σ^2
よって、yのサンプル間の分布の関係（＝yの同時分布）が、xのサンプル間の関係で表せた！
サンプリング
　bの値を、平均0分散σ^2の正規分布に従うようにランダムに選ぶ（＝サンプリング）
　→x,yをプロット
非線形へ応用する場合、カーネル関数を使用（SVRと同様）
概要では「クロスバリデーションがいらない」とあるが、
最後のハイパーパラメータの決め方では、含まれている
1.事前知識から決定
2.クロスバリデーション
3.最尤推定　※もっとも一般的

色々調べてはみたけれど、正直まだ分からない
http://matlabexpo.com/jp/2016/proceedings/c3-machine-learning-for-data.pdf
https://library.naist.jp/mylimedio/dllimedio/showpdf2.cgi/DLPDFR006108_P1-44
http://scikit-learn.org/stable/modules/gaussian_process.html
</div>

ガウス過程による回帰(Gaussian Process Regression, GPR)
～予測値だけでなく予測値のばらつきも計算できる！～ 
https://datachemeng.com/gaussianprocessregression/
これを読んでも、イマイチ理解しきれなかったのでググった

https://www.slideshare.net/KenjiUrai/explanation-of-gpr
線形回帰モデルでまず考える → 後にカーネル法で非線形に応用
y = X^T w + e(noise) = f(X) + e

入力X,係数wが得られた時に出力yが得られる確率 p(y|X,w)
= Π_i p(y_i|X_i,w)
 (以降、p を ガウス分布N(ν,σ)として、式変形)
= N(X^T w, σ_n^2 I)

事前分布 w ~ N(0, Σ_0)と仮定すると、
入力X,出力yが得られた時に係数wが得られる確率 p(w|X,y)
~ N(w', A^-1)となる
w' = A^-1 b
A = XX^T/σ_n^2 + Σ_0^-1
b = Xy/σ_n^2 
データセットX,yから、Xとyの対応を表す関数の分布が得られたことになる

以上の結果を用いて、新しいデータX*に対する予測値f(X*)の確率分布p(f(X*)|x*)を計算すると、
平均E[f(X*)] = X*^T w'
分散V[f(X*)] = X*^T A^-1 X*
これらは、
k(X,X') = X*^T Σ_0 X'
K       = X ^T Σ_0 X
k*^T    = X*^T Σ_0 X
を用いて次のように表すことができる
E[f(X*)] = k*^T (K + σ_n^2 I)^-1 y
V[f(X*)] = k(X*,X*) - k*^T (K + σ_n^2 I)^-1 k*
よって、p(f(X*)|x*) ~ N(E[f(X*)],V[f(X*)])を予測できる

サンプル計算より、
sin(x), sin(x1,x2)を再現
トレーニングデータが増えれば、V[f(X*)]が減少し、再現性も高くなる

※カーネル関数はこのスライドでは出てこない
k(X,X') = X*^T Σ_0 X' = Kernel(X,X) Σ_0
ここでは、Kernel(X,X)が線形のまま(サンプルは除く)
カーネル関数を変えることで、非線形に対応できる
あるいは、A = XX^T/σ_n^2...のところ？


[1b4] 機械学習の概要
http://matlabexpo.com/jp/2016/proceedings/c3-machine-learning-for-data.pdf
パラメトリック回帰
    モデル式を仮定して、データにフィットするようなパラメータを探索
    データの傾向や関数がある程度わかっている場合に有効
    ex)線形回帰
ノンパラメトリック回帰
    関数の形を定めない
    事前に関数がわからない場合や、データの数が少ない場合に有効
    ex)ガウス過程回帰、決定木、サポートベクトル回帰
？ガウス過程回帰はノンパラメトリック？線形のを仮定したのでは？

[1b5] パラメトリックとノンパラメトリック
K近傍法の特徴について調べてみた
https://qiita.com/Tokky0425/items/d28021eb1c2a710ec9f9
ノンパラメトリック
関数の形があらかじめ想定されているものをパラメトリック、そうでないものをノンパラメトリック
    メリット
        柔軟にモデルを作れるので、パラメトリックよりパフォーマンスに優れる
    デメリット
        データの量が少ないと効果を発揮しにくい
            ！！！　[1b4]のと真逆！！！
        パラメーターの数が多くなりがちなので訓練に時間がかかる（K近傍法はその限りではない）
        過学習(overfitting)しやすい
怠惰学習　※ノンパラメトリックとは同義ではない。
教師データから何かしらの新しい方程式を作り出す(= eager learner)のではなく、
単純に教師データを丸暗記する(= lazy learner)タイプの学習アルゴリズム
ノンパラメトリックでも、関数の形を「予め想定しない」だけで、学習した後は決まる。


[1b6] ガウス過程回帰とノンパラメトリック
ガウス過程（ノンパラメトリック回帰）
http://nobunaga.hatenablog.jp/entry/2015/09/15/221358
！！！！ これを読んで、ようやく分かってきた気がする
ノンパラメトリック全般
　ノンパラメトリック回帰ではモデル式を仮定せず，データだけから推定する．
具体的にはデータだけから推定する方法として近傍との関連性を考えて推定する．
例えば移動平均で内挿する方法や，データ間の距離の逆重み平均で内挿する方法もノンパラメトリック回帰と言える．
ここで，ノンパラメトリック回帰で重要となるのが，推定するモデルの滑らかさ，

　つまり近傍との関係性をどんな風に仮定するか？この仮定がカーネルと呼ばれるもの．
例えば，距離の逆数で指数的に関係性が依存する場合として，　exp(-(x1-x2)^2)などが良く使われる

　ノンパラメトリック回帰とパラメトリック回帰の大きな違いとして，
データを保持しておくかどうかがある．パラメトリック回帰では回帰係数が求まればデータ自体は捨ててもいい．
対して，ノンパラメトリック回帰はデータから決めるんだから当然とっておかなければならない．

　同様に，ノンパラメトリック回帰では予測するのに必要な計算量もデータ数が多くなるほど多くなる．
パラメトリック回帰は回帰係数を求めるのにデータが多くなるほど時間はかかるが，
モデルさえ求まれば予測自体はデータ数に依存しない点が異なる．

ガウス過程回帰について
　本来やりたいことは，「観測されたn個のデータを生成する現象のモデル（関数）は？」ということ．
そこで，観測されたn個のデータがn変量正規分布からの一組の実現値であると見る
（つまりn次元の多変量正規分布のサイコロを一回振って生成されるn個のデータが観測されたとみる）んだと思う．

　で，そんなn変量の正規分布は？っていうのを，
各データを平均値，各データ間の分散を共分散行列とする多変量正規分布から生成された，と考える．


[1b7] ガウス過程回帰とノンパラメトリック２
パラメトリックとノンパラメトリックの狭間
http://d.hatena.ne.jp/risuoku/20121210/1355137954
データ間の「関係」
データ間の関係が推定に使えることがわかりました。
ここで、データ間の関係を数値化する関数を準備します。
このような関数は、「カーネル関数」という術語で知られています。
例えば、$k(x1,x2)=exp(-￥beta{||x1-x2||}^2)$のようなカーネル関数を定義すると、
x1とx2は近いほど関係が強く、遠いほど関係が弱いことを定量的に扱えます。
# カーネル関数の意味が分かった！？

ガウス過程
前節では、データ間には関係があることをみて、関係を数値化するためにカーネル関数を導入しました。
ここで、推定したい関数fについて考えます。
重要なポイントからいうと、fというのは、ガウス過程に従う確率変数です。
関数fの形を明示的に与えない代わりに、関数の値に確率的な制約
（結局のところ、ノイズが正規分布に従う仮定に基づきます）をつけています。
fがガウス過程に従うことを形式的に表現すると、$(f(x_1),f(x_2),...,f(x_n))=N(0,C)$ となり、
fの値がn次元多変量正規分布に従うことになります。（わかりづらいですが、平均の0はn次元です）
ここで重要なポイントが、共分散Cの意味です。
共分散行列の各要素は各変数の関連度を表していました。
各変数というのは、推定したい関数の値ですから、たしかにデータ間の関係を考えています。
実は、共分散行列の各要素の値にカーネル関数を使うと、上手く関数fを推定できることが知られています。
細かい話は省きますが、p(訓練データ)とp(訓練データ,新しい入力)が計算できることから、
p(新しい入力|訓練データ)が得られます。

# 私の今のところの理解
ガウス過程回帰では、関数y=f(x1,x2,...)というよりも確率分布p(y|x1,x2,...)を求めている？
その確率分布を計算するうえで、関数を線形と仮定(のちにカーネル関数で一般化)しているのか？
    関数というよりは関数＋ノイズ、つまり、y = f(x) + 正規分布のノイズ
    と仮定している。この場合、予測値を確率分布で考える。
ある意味、関数の形を決めていると言えなくもない。
関数でなく、確率分布なので、予測値ではなく、、予測値の平均とその分散、を得る。
関数f(x)+ノイズN(0,σ)と仮定しているので、予測値の平均f(x)、分散σとなる？


[1b8] サンプル計算
sklearn.gaussian_process.GaussianProcessRegressor
http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html

Gaussian Processes regression: basic introductory example
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html
ハイパーパラメータの最適化は、GridSearchCVではなく、
パラメータoptimizerで指定するらしい。
パラメータがカーネル関数の中にもあるため、モデルのパラメータを最適化するのとはやり方が異なるのだろう。

[1b9] Tc計算に応用
先述の通り、ハイパーパラメータの最適化にまつわるルーチンが異なる。
また、.predictメソッドで、分散も出力するため、書き方を変えなければならない。
よって、計算プログラムは今までのまとめたものとは、別に作成する。

C:  RMSE, MAE, R^2 = 5.108, 3.285, 0.983
C:  RMSE, MAE, R^2 = 8.236, 5.620, 0.956  Random Forest
C:  RMSE, MAE, R^2 = 5.854, 3.615, 0.978  SVR
C:  RMSE, MAE, R^2 = 9.528, 5.602, 0.941  kNN
C:  RMSE, MAE, R^2 = 33.894, 25.541, 0.257  RR
C:  RMSE, MAE, R^2 = 33.918, 25.422, 0.256  LASSO
C:  RMSE, MAE, R^2 = 33.913, 25.446, 0.257  Elastic Net
一番性能がいい！？
yy-plotやエラーのヒストグラムを見ても、特別異常はない

DCVやy-randamizationは省略

計算時間 56.21 seconds 
そこそこ早い　線形(RRなど)よりは遅く、RFやSVRよりは早い

AD内部のTc top10
formula,P,Tc,std,AD
H3S,200,197,1,1
H3S,250,183,0,1
H3S,300,178,0,1
CaH6,150,167,1,1
AsH8,400,143,0,1
AlH5,250,139,1,1
H3Se,200,115,0,1
H3Se,250,110,0,1
H3Se,300,109,0,1
BiH5,300,103,1,1


[1c] todoの処理
[1c1]
[todo] -> [stop] 
・Tcの計算には使わなさそう
・難しい

どんなデータでも(※)線形分離可能にしてしまう技術，Vanishing Component Analysis(ICML 2013)を紹介してきました
http://conditional.github.io/blog/2013/07/10/vanishing-component-analysis/



[1d] Deep Forest
[1d1] arxiv
Zhi-Hua Zhou, Ji Feng
"Deep Forest: Towards An Alternative to Deep Neural Networks"
https://arxiv.org/abs/1702.08835

[1d2] 出版された
https://www.ijcai.org/proceedings/2017/497
https://doi.org/10.24963/ijcai.2017/497

[1d3] 解説1
https://github.com/arXivTimes/arXivTimes/issues/235
論文をまとめようとした記事(途中で止まった？)
一言でいうと
ハイパパラメタのチューニングがほぼ不要な決定木のアンサンブルメソッドであるgcForestを提案。
構造は下層での複数のforestsからの出力をconcatし、
それを次の層の複数のforestsの入力に用いるというカスケードモデル。
ディープラーニングと比較して、計算資源, 必要な教師データ数が少なくてよく、
異なるドメインから生成されたデータに対しても頑健、並列化が容易という利点がある。

[1d4] 解説2
Deep forest
https://www.slideshare.net/naotomoriyama/deep-forest-74148319
解説のスライド
5. 論文のサマリ
ニューラルネットと同等な計算精度を持つアンサンブル決定木モデルを用いて、ニューラルネットの代替えを提案 
* パラメータ調整の簡易化がもたらす計算速度
* GPUリソースを必要としない低リソース
* 木構造の性質である簡易な並列化
* 小規模なデータから効率よく学習可能 

6. 事前知識-決定木関連 
決定木 -規則を設けて識別境界を設ける 
アンサンブル学習 -複数の決定木の結果を合わせて識別器を作る 
バギング -複数の識別器の結果から多数決で出力を決める 
ランダムフォレスト -バギングに制限を設けて分散を抑える

7. 事前知識-ニューラルネットワーク
中間層（隠れ層）をもつ全結合の有向グラフ
強み
* 単調な非線形表現を積み重なることで、 複雑な非線形表現（合成関数）を獲得
* パターン認識問題が得意
弱み
* 現在のデータだけでなく、過去の情報が必要な 問題は苦手
* 初期に学習された情報は忘れてしまう

8. ニューラルネットのココがだめ
* 大量なデータがないと精度が出ない
* ↑このデータを作るコストが高い
* 大量な計算資源を要する
* パラメータが多く、そのチューニングが大変 
* 人の手を介さずに特徴を抽出できるのが利点と言われるが、 実際のところかなり細かいチューニングが必要
* 論理的な理解が難しい

9. gcForestのココが素晴らしい 
※gcForestは本論文が提案するアンサンブル決定木モデル
* 少量なデータで学習できる
* パラメータが少なく、チューニングが簡易
* CPUのみのgcForestとGPUありのニューラルネットの 計算速度が互角
* 論理解析が簡単

10. 以降gcForestの概念を論文に沿って 説明して行きます

11. gcForestの概念
* gcForestはmulti-Grained Cascade forestの略
* Cascadeはこんなイメージ 
上流から下流へたくさんの段を構成し情報が流れる 
写真では不変な水が流れるが、本モデルでは段ごとにそれぞれ計算がなされる 
http://www.geocities.jp/emkj20002000/newpage53.htmlより引用

12. Cascade forest 
３クラス分類の問題をgcForestで解くケースを例を考える 

13. Cascade forest
1.学習データ入力
2.前ページのような 複数のcascade(段)を構築
3.各段ではそれぞれ決定木とランダムフォレストの２種類の分類器を設ける
4.出力段では３クラスそれぞれの確率を出力 

14. Cascade forest 
一つの段の中で起きている計算は下記のようになる 
1.それぞれの決定器（ランダムフォレストで計算を行い、出力クラスの推定を行う
2.最終出力では全てのフォレストの出力の平均を取る
 ※学習時には、過学習を防ぐためのクロスバリデーションは行われる

15. Multi-Grained Scanning 
RNN(LSTM)の入力ミニバッチとCNNの畳込み概念を用いて
学習データから特徴量の抽出を行うことでさらに近い性能を引き出す
1.時系列データを一定区間ごとにずらしてミニバッチを作る。 （どちらかと言えばword2vecに近い）
2.おなじみのCNNプーリング

16. gcForestの処理流れ 
1.Multi-Grained Scanningで前処理 ここでは3種類のプーリングを合算 
2.Cascade forestで出力計算

17. ベンチマーク 
MNISTの画像分類 ORLデータセットの顔認識
GTZANデータセットの音楽分類 生体データ（手の動き）認証
その他多数あり

18. まとめ
* 決定木ベースのアンサンブル学習であるランダムフォレストを更に多層（段）で構築したgcForestを紹介した
* 著者のニューラルネットへの代替えに対する情熱が凄まじく、理論的に可能であることも感覚的に理解できる
* 利便性と低リソース観点から、XGBoostに次ぐ学習器として期待できそう（個人感）
* PythonかRで実装できたらKaggleで流行ると思われる 


[1d5] 解説3
https://www.slideshare.net/harmonylab/deep-forest-towards-an-alternative-to-deep-neural-networks
2. 論文概要
現在の深層モデル
 – ニューラルネットワーク
Deep Forest(gcForest)を提案
 – 決定木アンサンブルアプローチ
 – 広範囲のタスクでDNNと競える高い性能
 
4. Deep Neural Network
強力 
様々なタスクで成功
 – 特に視覚、音声情報を含むタスク
欠点
 – 訓練データが大量に必要
       現実のタスクでは、ラベル付けされたデータが不足
       ラベリングのコストが高い
 – モデルが複雑
       訓練には強力な計算設備が必要
       ハイパーパラメータが多い上に、学習性能がその調整に依存
       理論的な解析が難しい
5. Deep Neural Network
表現学習
 – 生の入力から特徴を抽出する
 – DNNにとって重要
大規模なデータを活用するためには、学習モデルのキャパ シティも大きい必要がある
-> これらの特性を他の学習モデルに与えることが出来れば、 DNNと競える性能を達成することができるのでは？
-> gcForestを提案 

6. gcForest
カスケード構造
 – 表現学習を可能に
 – カスケードレベル数は適応的に決定可能
       モデルの大きさ(複雑さ)を適応的に決定
Multi-grained scanning
 – 表現能力を高める
7. gcForest
ハイパーパラメータ数が少ない
 – デフォルトで様々なタスクに対応可能(堅牢)
DNNに対して競争力の高い性能を持つ一方で訓練時間は短い
この論文の範囲を超えるけど理論的分析も簡単

8. カスケード構造
各レベルは、その前のレベルで処理された特徴情報を受け取り、その処理結果を次のレベルに渡す
9. カスケード構造
多様性がアンサンブル構築にとって重要
 – 異なる種類のフォレストを使用
 – 図は各レベルが2つのランダムフォレスト(黒)、2つの完全ランダム決定木フォレスト(青)で構成
10. カスケード構造
インスタンスが与えられると、インスタンスが属するノードのトレーニング例のクラスごとの割合をカウントし、全ツリーの平均をとる 

11. 各フォレストの学習
完全ランダム決定木フォレスト
 – 500個の木を含む
 – 決定木の各ノードで分割のための特徴をランダムに選択
 – 各ノードが同じクラスのインスタンスだけとなるまで成長
ランダムフォレスト
 – 500個の木を含む
 – 𝑑個の特徴を候補としてランダムに選択し、
   その中から最もGini係数の良いものを分割に使用することで作成（d:入力特徴数）
12. 各フォレストの学習
K-fold cross validation(実験ではK=3)
カスケードの拡張
1. 新しいレベルを拡張
2. カスケード全体の性能を検証セットで推定 
3. 性能が大幅に向上しなければ、学習終了
 – カスケードレベルが自動的に決定される
 – DNNがモデルの複雑さが固定されているのとは対照的に、適切な時に学習を終了し、
   モデルの複雑さを適応的に決定
 – 大規模なものに限らずに、様々なスケールのデータに適用可

13. Multi-Grained Scanning 
特徴間の関係を扱う目的 – CNN、RNNに触発

以下、システムや実験結果などはメモ省略

40. 結論
gcForestは実験でDNNと競える性能を示した
gcForestはハイパーパラメータが少ない
また同じパラメータ設定でも様々なタスクで優れた性能が得られる 

[1d6] 非公式翻訳
Deep Forest :Deep Neural Networkの代替へ向けて
https://qiita.com/de0ta/items/8681b41d858a7f89acb9
論文の翻訳記事
コメントも読む

[1d7] 続編
AutoEncoder by Forest
Ji Feng, Zhi-Hua Zhou
https://arxiv.org/abs/1709.09018

[1d8] 理解、疑問
つまるところ、アンサンブルのアンサンブル
人工ニューロンの代わりに、ランダムフォレストを用いた、ディープラーニング
入力、中間層におけるランダムフォレストは、教師データの出力を何にする？
    それが結果的に、特徴量を生成することに対応するのだろうが...

以下、翻訳論文より引用
<div>
2.3 全体の流れとハイパーパラメータ
図4はgcForestの全体の流れを要約している．
元の入力に400個の特徴があり，multi-grained scanningに3種類のウィンドウサイズが使われるとする．
ウィンドウサイズ100は元の訓練例それぞれから301個の100次元特徴ベクトルを生成する；
m個の訓練例があって，このスライドウィンドウが301×m個の100次元の訓練例を生成することになる．
これらのデータは完全ランダム決定木フォレストとランダムフォレストを訓練するのに使用されるが，
それぞれは30個の木を含む．
予測すべきものが3クラスあるとする；
各フォレストは3次元クラスベクトルを各301個のインスタンスに対して出力する．
2つのフォレストから生成されたすべてのクラスベクトルを結合した後，1つの1806次元特徴ベクトルが得られる．

図4
Raw input Features 400-dim
->
Multi-Grained Scanning
    100-dim --> Forest A1 -> 903-dim
            +-> Forest B1 -> 903-dim
    200-dim --> Forest A2 -> 603-dim
            +-> Forest B2 -> 603-dim
    300-dim --> Forest A3 -> 303-dim
            +-> Forest B3 -> 303-dim
-> concatenate -> 3618-dim -> 
Cascade Forest
    level 1
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    ---------> 3618-dim +-> 3630-dim -> 
    ...
    level N
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    ---------> 3618-dim +-> 3630-dim -> 
    
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +
    -> Forest  -> 3-dim +-> 12-dim ->
    Ave. 3-dim ->
    Max. 1-dim = Final Prediction
</div>


Deep Learning : Bengio先生のおすすめレシピ
http://conditional.github.io/blog/2013/09/22/practical-recommendations-for-gradient-based-training-of-deep-architectures/

4時間で「ゼロから作るDeep Learning」を読んで自分で動かしてみた
https://qiita.com/octas/items/6db284459506a746a33a
