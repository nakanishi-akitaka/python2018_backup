# -*- coding: utf-8 -*-
"""
Created on Thu Nov 22 00:30:09 2018

@author: Akitaka
"""

[1a2] midkNNについて
https://datachemeng.com/midknn/

以前からの理解
    ダブルクロスバリデーション/二重交差検証/DCVの意義
    https://datachemeng.com/modelvalidation/
    https://datachemeng.com/doublecrossvalidation/
    データ数が大　→　トレーニング + バリデーション + テスト
    データ数が中　→　クロスバリデーション + テスト
    データ数が小　→　ダブルクロスバリデーション
    CVは2,5,10-foldが一般的
今回
「データ数が中～小でクロスバリデーションができない」or「多すぎてクロスバリデーションしたくない」とき、
バリデーションデータをトレーニングデータの中点として、新しく「作成」する
中点をうまく再現するようにハイパーパラメータを最適化する

サンプルコード
https://github.com/hkaneko1985/midknn
https://github.com/hkaneko1985/midknn/blob/master/demo_midknn_svr.ipynb
SVRのハイパーパラメータをmidknnで最適化
γだけグラム行列で最適化　→　C,εをグリッド最適化

特別はやい訳ではない？


[todo]
http://neuro-educator.com/mlcontentstalbe/
ENとLASSOは回帰係数が0になりやすい　→　一部の特徴量が重要な場合に使用？

https://datachemeng.com/rrlassoen/
回帰係数の絶対値の和を最小化する場合、0になりやすい
RRのように二乗和を最小化する場合は、そうでもない



[1b] 水素化物Tc計算 & my_library.py　アップデート
[1b1] DCVが「テストデータに対するスコア」の平均になるハズ
実際にチェック
EN
C:  RMSE, MAE, R^2 = 31.717, 21.600,  0.722
CV: RMSE, MAE, R^2 = 40.661, 28.177,  0.529
TST:RMSE, MAE, R^2 = 28.660, 19.863,  0.675
DCV:RMSE, MAE, R^2 = 35.944, 24.439,  0.611 (ave)
DCV:RMSE, MAE, R^2 =  0.672,  0.443,  0.015 (std)
rnd:RMSE, MAE, R^2 = 56.645, 41.630,  0.033 (ave)
rnd:RMSE, MAE, R^2 =  0.435,  0.389,  0.015 (std)

GB
C:  RMSE, MAE, R^2 =  8.782,  6.019,  0.978
CV: RMSE, MAE, R^2 = 24.664, 15.939,  0.829
TST:RMSE, MAE, R^2 = 20.469, 10.970,  0.820
DCV:RMSE, MAE, R^2 = 22.323, 13.698,  0.848 (ave)
DCV:RMSE, MAE, R^2 =  2.466,  0.932,  0.033 (std)
rnd:RMSE, MAE, R^2 = 54.197, 40.005,  0.115 (ave)
rnd:RMSE, MAE, R^2 =  0.644,  0.416,  0.021 (std)

kNN
C:  RMSE, MAE, R^2 = 19.548, 11.873,  0.876
CV: RMSE, MAE, R^2 = 35.609, 21.331,  0.588
TST:RMSE, MAE, R^2 = 27.042, 18.720,  0.827
DCV:RMSE, MAE, R^2 = 24.704, 14.798,  0.813 (ave)
DCV:RMSE, MAE, R^2 =  3.292,  1.740,  0.053 (std)
rnd:RMSE, MAE, R^2 = 51.278, 37.899,  0.207 (ave)
rnd:RMSE, MAE, R^2 =  1.410,  1.277,  0.043 (std)

LASSO
C:  RMSE, MAE, R^2 = 31.717, 21.600,  0.722
CV: RMSE, MAE, R^2 = 35.955, 24.374,  0.642
TST:RMSE, MAE, R^2 = 32.823, 23.666,  0.488
DCV:RMSE, MAE, R^2 = 34.761, 23.790,  0.636 (ave)
DCV:RMSE, MAE, R^2 =  0.787,  0.432,  0.017 (std)
rnd:RMSE, MAE, R^2 = 56.800, 41.796,  0.028 (ave)
rnd:RMSE, MAE, R^2 =  0.276,  0.305,  0.009 (std)

PLS
C:  RMSE, MAE, R^2 = 31.725, 21.764,  0.730
CV: RMSE, MAE, R^2 = 41.162, 27.294,  0.545
TST:RMSE, MAE, R^2 = 34.378, 23.574,  0.148
DCV:RMSE, MAE, R^2 = 36.882, 25.167,  0.590 (ave)
DCV:RMSE, MAE, R^2 =  1.488,  1.089,  0.033 (std)
rnd:RMSE, MAE, R^2 = 57.092, 42.067,  0.018 (ave)
rnd:RMSE, MAE, R^2 =  0.234,  0.247,  0.008 (std)

RF
C:  RMSE, MAE, R^2 =  9.637,  6.386,  0.969
CV: RMSE, MAE, R^2 = 26.108, 15.677,  0.772
TST:RMSE, MAE, R^2 = 17.197, 11.693,  0.936
DCV:RMSE, MAE, R^2 = 22.928, 14.124,  0.840 (ave)
DCV:RMSE, MAE, R^2 =  2.612,  0.916,  0.037 (std)
rnd:RMSE, MAE, R^2 = 40.741, 28.416,  0.499 (ave)
rnd:RMSE, MAE, R^2 =  1.575,  1.037,  0.039 (std)

RR
C:  RMSE, MAE, R^2 = 28.480, 19.581,  0.758
CV: RMSE, MAE, R^2 = 33.089, 23.677,  0.673
TST:RMSE, MAE, R^2 = 38.838, 22.861,  0.524
DCV:RMSE, MAE, R^2 = 35.231, 23.758,  0.625 (ave)
DCV:RMSE, MAE, R^2 =  1.949,  0.778,  0.043 (std)
rnd:RMSE, MAE, R^2 = 55.073, 40.812,  0.086 (ave)
rnd:RMSE, MAE, R^2 =  0.586,  0.382,  0.019 (std)

GPR
C:  RMSE, MAE, R^2 =  5.509,  3.331,  0.992
CV: RMSE, MAE, R^2 = 56.076, 33.143,  0.133
TST:RMSE, MAE, R^2 = 43.408, 27.074,  0.099
DCV:RMSE, MAE, R^2 = 61.109, 32.121, -0.187 (ave)
DCV:RMSE, MAE, R^2 = 14.387,  2.163,  0.671 (std)
rnd:RMSE, MAE, R^2 = 36.426, 22.454,  0.598 (ave)
rnd:RMSE, MAE, R^2 =  2.459,  1.540,  0.054 (std)

SVR
C:  RMSE, MAE, R^2 = 10.612,  4.859,  0.961
CV: RMSE, MAE, R^2 = 36.325, 22.530,  0.540
TST:RMSE, MAE, R^2 = 34.321, 21.179,  0.764
DCV:RMSE, MAE, R^2 = 34.173, 21.106,  0.645 (ave)
DCV:RMSE, MAE, R^2 =  3.180,  0.898,  0.066 (std)
rnd:RMSE, MAE, R^2 = 50.194, 27.880,  0.240 (ave)
rnd:RMSE, MAE, R^2 =  1.212,  0.883,  0.036 (std)


大雑把に言えば、その通りの傾向。当然ではあるが。
GPRは平均に対して分散が明らかにデカい。
過学習を起こしやすい、と考えている。



[1b2] ADの設定
kNNで計算する場合、
https://datachemeng.com/knn/
> kをどうするかは試行錯誤, k=5, 10が一般的
とのことなので、ADを変更して、Tcの候補を調べるか、
ADを計算する際に、k=1,2,3,4,5,...でのADをすべて一括で計算するのも方法の１つ

また、X_train, X_testの定義を変えたため。
    y_appd = ad_knn(X_train, X_pred)
も変更する必要あり

→

プログラム作成　完了
# ver.1
neigh = NearestNeighbors(n_neighbors=n_neighbors+1)
neigh.fit(X_train)
dist_list = np.mean(neigh.kneighbors(X_train)[0][:,1:], axis=1)
# neigh.kneighbors[0] = サンプルとの距離を近い順に
# neigh.kneighbors[1] = サンプルのindexを近い順に 
#　自分との距離なので、一列目neigh.kneighbors(X_train)[0][:, 0] = 0.0になる　→　とばす
dist_list.sort()
ad_thr = dist_list[round(X_train.shape[0] * r_ad) - 1]
neigh = NearestNeighbors(n_neighbors=n_neighbors)
neigh.fit(X_train)
dist = np.mean(neigh.kneighbors(X_test)[0], axis=1)
y_appd_test1 = 2 * (dist < ad_thr) -1

# ver.2
dist_matrix_train = cdist(X_train, X_train)
dist_matrix_train.sort()
dist_list = np.mean(dist_matrix_train[:, 1:n_neighbors+1], axis=1)
#　自分との距離の行列なので、一列目dist_matrix_train[:, 0] = 0.0になる　→　とばす
dist_list.sort()
ad_thr = dist_list[round(X_train.shape[0] * r_ad) - 1]
dist_matrix_test = cdist(X_test, X_train)
dist_matrix_test.sort()
dist = np.mean(dist_matrix_test[:, 0:n_neighbors], axis=1)
y_appd_test2 = 2 * (dist < ad_thr) -1

以上の2バージョンをnlist= 1 ~ 10まで試しても
print(np.allclose(y_appd_test1,y_appd_test2))
はすべてTrueになったので、どちらでも全く同じ

ref:
http://gratk.hatenablog.jp/entry/2017/12/10/205033
https://deepage.net/features/numpy-zeros.html
https://datachemeng.com/wp-content/uploads/assignment15.py
20180726
20180808


テストした限りでは、
5925個中
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 3906個
[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.] 1318個
残り = 5925 - 3906 - 1318 = 701個
のようにほぼ2極化
残りは、kの範囲によってADの内外が変わる

X_train inside AD / all X_trainの比率を以下の通り変えた場合
r_ad = 0.99 -> 0.90
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 229個
[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.] 3988個
残り = 5925 - 229 - 3988 = 1708個

r_ad = 0.99 -> 0.997
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 5165個
[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.] 520個
残り = 5925 - 5165 - 520 = 240個
2極化が進行

今までのは、r_ad=0.90であった
AD外だと判定したものが、あまりにも多いのでは？




[1c] todo処理
[todo] -> [doing]
    水素化物Tc予測プログラム 更新の余地
    * 説明変数の増加 + PCAなど
-> 実行中

    * Tc=0Kとの分類＋回帰では？
-> 予定
そもそもTc = 0Kのデータを集める
安定に存在できない組成比があるのをTc=0Kとしてデータベースにいれる？

[todo] -> ????
考察
kNNやOLSでもいいから、実現できないか？
個性的な人工知能をつくる
https://datachemeng.com/uniqueartificialintelligence/
1.アウトプットの信頼性も一緒に返してくれる
    ADorデータ密度がその一種。GPRやアンサンブル学習なら、予測値と同時に分散を出力できる。
2.どんなデータがあれば、信頼性が上がるか教えてくれる
    ADでないが、ADに近い領域を表示できればいいのでは？
3.逆解析で複数のアウトプットを返すときに、補足情報によって順位づけしてくれる
    逆解析する場合、異なるxに対して同じyが出力されることがある
    その際、1.によりyの信頼度も出力すれば、それでランク付けできる
4.どうしてそのアウトプットになったか教えてくれる
    補完みたいな手法、ノンパラメトリック回帰の場合は、近くのデータがそうだから、としか言えない(kNNなど)
        強いて言えば、近く(と機械が判断した)データを表示するぐらいか
    モデルを設定する、パラメトリック回帰の場合は、理解しやすい
    ただし、ニューラルネットワークのような複雑すぎるものは厳しい

1.4.ができるかは手法次第な部分がある
1.ができれば、3.はほぼ自動的にできる
AD, データ密度が計算できれば、2.はすぐできる
一応、kNNでそれなりのことはできるっぽい



[1d] 考察
機械学習って、結局はただの補完じゃないの？
線形であれ非線形であれ
線形補完の範囲をデータのある領域全部でやる　→　最小二乗法ともいえる

20180919のPLS, LWPLS考察より
PLSは単純な線形モデル
LWPLSは、予測したいデータの近くの学習データをなるべく再現するよう線形モデルを組む
小さい区間での線形補完を繰り返しやってるようなものでは？

少なくとも、最初からモデルの形を仮定しないノンパラメトリック回帰はほぼ補完と同じ印象
モデルの形を仮定するの、パラメトリック回帰は、全区間での補完とも言える？
SVR：カーネル関数の形で補間　ガウス関数など
ランダムフォレスト：ある範囲にあるデータを線形y=constantで近似したようなもの

結局は補完なのであれば、特別目新しい発見はないのか？
画像識別のような、説明変数が多いもので、
かつ、単純な補完が難しいものであれば、機械学習である意味がありそう



11/20
第5回 競馬予測の入力となる特徴量を作る
https://alphaimpact.jp/2017/02/09/make-feature/
今回はカテゴリデータを主とした特徴量の作成方法について解説しましたが、その重要性を感じていただけたでしょうか。
すでに第5回目だというのにまだ予測ロジックについての話が出てこないではないかと思われてる方も多いかと思いますが、
入力となるデータ作りはただの手の運動ではなく入念な設計が必要となるクリエイティブなプロセスで、
実際のAlphaImpactの開発では予測ロジックの作成以上に時間を要しています。 
最強の競馬人工知能を作る一番の近道は最強の競馬特徴量職人になることだと言っても過言ではないでしょう。

説明変数の作成方法が大事？

野球、マネー・ボール　
新しい評価基準 = 説明変数 OPSを作成
自分で考えた結果

将棋
評価関数　あるいはその入力＝説明変数＝2,3,4駒関係
考えた結果？

ディープラーニングによる画像識別
説明変数は勝手に作成する　そのため、人間に解釈できる形とは限らない
人間は一切考えていない

物性物理学の場合は？
質量やら第一イオン化エネルギーやら電気陰性度などで作成される
将棋・野球同様に、自分で考えたもの

競馬や将棋、画像認識でもそうだが、これを特徴量にすれば絶対大丈夫というものはない
ただ、将棋や画像認識については、「最小限必要なライン」はハッキリしている
    駒の位置と持ち駒、画像データそのまま
物性物理学でいえば、原子の種類と個数？
第一原理計算は、原子の種類と位置だが...
どちらにせよ、圧力や磁場はなしとしている




11/21
みにくいアヒルの子の定理？

20180628<div>
[1a3] 醜いアヒルの子の定理
https://en.wikipedia.org/wiki/Ugly_duckling_theorem
ここの図が一例
「先頭にいる」「色が白い」の２つの特徴量で３羽のアヒルを区別してる

例
特徴量
＝白or黒、♂or♀の２つ

識別可能な数
＝白♂　黒♂　白♀　黒♀の４つ
＝ベン図の最小区域
　※白♂一号　白♂二号は「同じ」≠「似てる」

記述ver2
＝白♂　白♀　黒♂　黒♀
　白♂∪白♀（＝白）　白♂∪黒♂（＝♂）　白♂∪黒♀＋それらの否定＋全部の１５個
＝ベン図で取り得る区域すべて＝最小区域の組み合わせ

記述例ver2
白♂＝1000-111000-0111-1
白♀＝0100-100011-1011-1
黒♂＝0010-010101-1101-1
黒♀＝0001-001110-1110-1

共通項目ver2
白♂白♀＝..00-1..0..-..11-1
白♂黒♂＝.0.0-.1..0.-.1.1-1
白♂黒♀＝.00.-..1..0-.11.-1
白♀黒♂＝0..0-..0..1-1..1-1
白♀黒♀＝0.0.-.0..1.-1.1.-1
黒♂黒♀＝00..-0..1..-11..-1
共通項は7個　共通して真は4個

つまり、白♂と白♀、白♂と黒♀はどちらも同じだけ似てる
日常的感覚では、二値特徴量である白or黒、♂or♀だけを見て、
前者は似てる、後者は似てない、と判断する
しかし、数学的には、白♂∪白♀（＝白）や、白♂∪黒♀という
「集合に属するか否か」で考えるため、両者を同等に扱う


一般的な場合、
d個の二値特徴量で、n=2**d匹のアヒルを区別できる
特徴量を使ってできる記述は2**n-1
※どれも含まないのは除外するため
2**n-1のうち、任意の2匹を同時に含む(どちらも真である)ルールの数は2**(n-2)

ver2に当てはめると
d=2, n=4, 記述15, 共通真4個

A or a, B or bの特徴量でAB Ab aB abの４個体を区別できる
記述をAB Ab aB ab　それぞれの真偽(=T or F)の組み合わせから作る
つまり、FFFFは除いて、TFFF, FTFF, ... , TTTT = 2**4 - 1
どの個体も、Tである記述の数は8, Fは7となる
共通項は、
共にT=1, 共にF=0として、
Tが1個の記述については2個の0
Tが2個の記述については1個の0,1個の1
Tが3個の記述については2個の1
Tが4個の記述については1個の1
合計7個であるが、共通して真(=1)となるものは4個

～の厳密な証明
nlp.dse.ibaraki.ac.jp/~shinnou/zemi1ppt/fujii-siryou.ppt
</div>


考察
任意の2つのサンプルの類似度が同じ
→　クラス分類学習不可能ということ？
    「似たサンプルは同じクラス」を前提として分類する場合、
    どのサンプルと比べても、類似度で差がつかなくなってしまう


http://ibisforest.org/index.php?醜いアヒルの子の定理
これは，各特徴量を全て同等に扱っていることにより成立する定理． 
すなわち，クラスというものを特徴量で記述するときには，何らかの形で特徴量に重要性を考えていることになる． 
この定理は，特徴選択や特徴抽出が識別やパターン認識にとって本質的であることを示唆している．


https://www.cresco.co.jp/blog/entry/2488/
…ちょっと詭弁っぽいですね。
実は、ノーフリーランチ定理が「考えうる全てのコスト関数に対して」というのがミソだったのと同じように、
これも「あらゆる種類の共通点 (や差異) を同じ重みで考える」というところがミソになっています。
実際には、人間が何かの類似性や差異を判断するときには、重要視するものとあまりしないものと、
意識的・無意識的に重み付けしているのです。
逆に、そうした適度な重み付けがないと、何かを類似度に基づいてグループ分けすることはできなくなってしまう、
というのが、この定理なのです。

機械学習を使って画像を分類する場合も、
要は「どの類似性・差異を重要視するか」ということを教師データを使って学習させています。
理屈から言うと、データセットに対してどんな分類の仕方でもできてしまいます 
(ノーフリーランチ定理からすると、学習しやすい分類の仕方となかなか学習が進まない分類の仕方はあるでしょう)が、
そこに何らかの価値観を入れ込むのです。

※ノーフリーランチ定理
「考えうる全てのコスト関数」というのがミソなのですが、それでも、それらを相手にしては、
凄いアルゴリズムとそうでもないアルゴリズムは変わらない、と言われてしまうと困ってしまいます。
最適値を探索するアルゴリズムを工夫することは無意味なのでしょうか? もちろん、そんなことはありません。

一般的に、ある領域の問題に対するコスト関数には、「ありとあらゆる」ものは出てきません。
何らかの特徴というか癖というか、そんなようなものがあります。
ノーフリーランチ定理が主張するのは、「ある領域の問題」に対する最適化アルゴリズムは、
その特徴や癖を事前知識としたりして、その領域に特化したものにせねばならない 
(それをぜんぜん違う領域に適用すると性能がでないかもしれない)、ということなのです。

機械学習にもやり方がいろいろありますが、「どれか一つのやり方で万事 ok」とはいかない、ということでもあります。
問題の領域に合わせて、やり方を選んだり工夫したりする必要があるのです。


http://www.kamishima.net/archive/mldm-overview.pdf
p.44ノーフリーランチ定理
p.46みにくいアヒルの子の定理
類似した対象が集まったクラスというものを実世界で見いだしているならば，
対象のある特徴を重視したり，逆に軽視したりしているということである．
そして，どの特徴を重視したり軽視したりするかは形式的な判断の範疇の外で決めている

予測問題にとって重要な特徴は限られているという仮説を支持
→ 次元削減，特徴選択，正則化の技法などが有効である理由


http://d.hatena.ne.jp/akkikiki/20130612/1371049768
自分が特に気に入っているのは「みにくいアヒルの子定理」の直観的解説の部分。
これは「人は自分の価値基準（＝事前知識orコスト関数と自分は解釈）を用いないと、
最適かどうかを判断できなくなる」と自分は解釈している。
あなたが人生で何を大切にするのか？を考えないと、現実でも機械学習の世界でも何も判断できないよ、
というメッセージが自分の心に刻み込まれた。

2013年6月15日追記：
上のメッセージはノーフリーランチ定理、すなわち全てのコスト関数（＝価値基準）
を最適にするアルゴリズムは存在しない、からも導出されている。
（ただ、このアルゴリズムは一定の制限があるので、詳しくは上記リンク参照。）


※上で言及しているのが以下のリンク
https://www.slideshare.net/mmktakahashi/ss-13694313
p.12あたりから、みにくいアヒルの子の定理
特徴選択・特徴抽出が識別・認識の本質 p.22

みにくいアヒルの子の定理
人間のパターン認識はエントロピー最小化に基づく情報の圧縮
認識とは、不要な情報を捨てること
思い切って不要な特徴量を捨てて、各特徴量に主観的な価値を与えなければ、
    人間はみにくいアヒルも普通のアヒルも白鳥も区別できなくなってしまう
特徴選択・特徴抽出が識別・認識の本質
パターン認識と機械学習においてはこれを忘れてはいけません
いろいろな特徴や次元を考えることはとても役に立つことですが
知っている特徴や次元を何も考えず全部手元に置いておくと人間は高次元ユークリッド空間で迷子になってしまいます
高次元ユークリッド空間は私たちの感覚と乖離しやすく
もしあなたが無限次元のユークリッド空間に放り出されたら見渡す限りすべてのものが、
    あなたからはるか遠くにあって何も見分けがつかなくなるようなそんな空間です
これは「次元の呪い」と言われています
いいものも悪いものも好きなものも嫌いなものも関係あるものも関係ないものも
見分けがつかなくなってしまう恐ろしい「呪い」ですね！
つまりあなたがアヒルと白鳥の雑多な集団の中から白鳥を探し出したいのなら
まずあなたはあなたが白鳥のどの特徴を選択・抽出して白鳥を白鳥と認識しているのかなるべく正確に見つける必要があります
「えー、でも僕がやりたいのはアヒルと白鳥の分類なんかじゃないよ」
「興味があるのはむしろ幸せの青い鳥の探し方だよ」
「一番いい結婚相手探し、一番いい仕事探し、一番いい治療法探し……」
「人生ってそういう最適解探しに満ちあふれているけど井の中の蛙には大海を知ることができないから」
「自分が知っている中で一番いいと思った選択肢は本当にいいものなのかという不安が常にある」
「安直で手近にある最適解じゃなくて『本物の』最適解を探すための普遍的なアルゴリズムがあったらそれこそ青い鳥だと思うけど」
「青い鳥はまだ見つかっていなくて」
「だから一番いいもの探しの旅がいつも全然終わらないんだ」
確かにそれは多かれ少なかれ誰もが抱きうる問題のようです
私も本日の「一番いい話題探し問題」には厳密解を出せそうにないのですが
そろそろ解の一つに収束する必要がありますからこれでいいと信じて本題に入ります

p127あたりから、ノーフリーランチ定理
コスト関数の極値を探索するどんなアルゴリズムも全てのコスト関数に適用すると平均性能は見分けがつかなくなる
コスト関数の極値を探索する最適化問題において全てのコスト関数に適用できる効率の良いアルゴリズムは存在しない
成立するポイントや前提
1.候補空間・評価値空間が有限で、コスト関数による評価値によって答えを出すという問題のみを考えています
    最適な「最適解探索アルゴリズム」探索アルゴリズムが存在しないとは厳密には言えません
2.この定理が使えるのは、過去の探索履歴によってコスト関数を推定しそのコスト関数による評価値を使って
    最適解を探索するようなアルゴリズムだけです
    ですから、過去にとらわれないランダムな当てずっぽうが偶然最適解になることは当然あるわけで
    フリーランチがないとしても棚からぼた餅に期待するのはダメだとは言っていません
3.この定理は試行錯誤の回数が少ないアルゴリズムを効率が良いアルゴリズムであるとみなしています
    つまり、同じ点を複数回評価するようなアルゴリズムについては適用されません
    過去に低く評価したものを敢えて再び評価してみるという一見無駄のあるアルゴリズムについては
    この定理は何も言っていないので
    ダメなのは知っているけどダメ元でもう一度やってみるというアルゴリズムは案外ダメとは限りません
4.この定理においてはアルゴリズム自身が探索空間や評価関数を変化させることはないと考えていますが
    自分の好みを自分で修正するアルゴリズムは良いアルゴリズムである可能性がありますし
    あなたの探索行動自体があなたの探索している世界を変えるという可能性は残されています
5.全ての問題に対する効率の平均ではなく全ての好み(コスト関数)に対する効率を平均していることに留意して下さい
    1.で出てきたように、この定理は候補空間・評価値空間が有限かつコスト関数による評価値によって答えを出す
    という組み合わせ最適化問題に関する定理で
    選択肢も評価値も無限にあったりそもそも評価ができない選択肢があるような非常に漠然としすぎた問題については
    何かを言うことはできません
この定理が述べているのはコスト関数を最小化するものを探したい場合すべてのコスト関数を満足させようとする
一見汎用性の高いアルゴリズムは専門性を失うということです
あなたが求めている青い鳥というのは、本当に全ての好み(コスト関数)に合う「汎用」アルゴリズムなのでしょうか？
正確な自分の好み(コスト関数)が自分でもよく分からなくても一番重要なのは自分の好み(コスト関数)であるはずです
事前の知識を用いて、コスト関数をある程度明確にして何を以って評価するのかをはっきりさせないといい予測や最適化はできません！
事前の知識・主観的な価値評価こそアヒルと白鳥のパターン認識という一見簡単そうな問題の鍵、でしたよね
色々なコスト関数が考えられるから欲張りたくなる気持ちはよく分かりますが
何らかの事前の知識や経験や勇気であまり重要ではなさそうなコスト関数を捨てることをおすすめします
このことは多くの困難な最適化問題において適切な妥協をするための鍵になります


http://www.ism.ac.jp/shikoin/training/dstn/pdf/C4.pdf
3. 機械学習の一般的な問題
• No-free-lunch定理 [Wolpert and Macready、1995]
「どの学習機械に対しても、ランダム推測の方が優れている
目的関数が少なくとも一つ存在する 」
• みにくいアヒルの子定理 [渡辺慧, 1969]
「課題から独立した特徴量/モデルは存在しない」



https://www.slideshare.net/dsuket/ss-57488780
p.29
アルゴリズム選択時の注意 
データ特性などに適したアルゴリズムを選択すること データの分布を前提に置いているものもある。 [10] 
ノーフリーランチ定理
コスト関数の極値を探索するあらゆるアルゴリズムは、
全ての可能なコスト関数に適用した結果を平均すると同じ性能となる 
あらゆる問題で性能の良い万能な学習アルゴリズムは存在しない — Wolpert and Macready、1995年


Google 辞めました
https://takeda25.hatenablog.jp/entry/20120511/1336746314
ところで、仕事を辞めて無収入になるので、「金くれ」というネット乞食サイトで乞食行為をしてみた。
http://kanekure.ssig33.com/takeda25
